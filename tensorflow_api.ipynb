{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic survival - TensorFlow api-based neural net.\n",
    "\n",
    "Taking https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/20b_tensorflow_api.ipynb and adapting the code to run in batch mode by making use of nbparameterise library.\n",
    "\n",
    "This workbook builds a neural network to predict survival on the titanic using the common framework TensorFlow (using keras).\n",
    "\n",
    "It can be run in standalone based on the parameter values defined in the first code cell.\n",
    "\n",
    "Alternatively, the notebook can be run in a batch mode by setting up the parameter values in \"batch_running_ipynb.ipynb\". That notebook will access the parameters in the first cell of this notebook and alter their value before running this notebook and saving it (with a unique name)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters that can be changed in a batch run\n",
    "The first code cell needs to contain all of the parameters that can be changed in a batch run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network structure\n",
    "number_hidden_layers =  1\n",
    "number_nodes = 1\n",
    "dropout = 0.5\n",
    "wide_and_deep = 1\n",
    "single_final_hidden_layer = 0\n",
    "\n",
    "# define the network training mode\n",
    "learning_rate = 0.01\n",
    "include_class_weights = 1\n",
    "calculation_batch_size = 32\n",
    "max_epoch = 3\n",
    "\n",
    "number_kfold = 5\n",
    "output_folder = \"output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the parameter values to create a string to be used as the model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \".\" with a \"-\"\"\n",
    "learning_rate_str =  str(learning_rate).replace(\"0.\", \"0-\")\n",
    "dropout_str = str(dropout).replace(\"0.\", \"0-\")\n",
    "\n",
    "model_name = (f\"model_{number_hidden_layers}_{number_nodes}_{dropout_str}_\"\n",
    "              f\"{wide_and_deep}_{single_final_hidden_layer}_{learning_rate_str}\"\n",
    "              f\"_{calculation_batch_size}_{number_kfold}_{max_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn for pre-processing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# TensorFlow api model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if folder and subfolder don't exist, create them\n",
    "if not os.path.exists(f\"{output_folder}\"):\n",
    "    os.makedirs(f\"{output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data if not previously downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to calculate accuracy measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(observed, predicted):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates a range of accuracy scores from observed and predicted classes.\n",
    "    \n",
    "    Takes two list or NumPy arrays (observed class values, and predicted class \n",
    "    values), and returns a dictionary of results.\n",
    "    \n",
    "     1) observed positive rate: proportion of observed cases that are +ve\n",
    "     2) Predicted positive rate: proportion of predicted cases that are +ve\n",
    "     3) observed negative rate: proportion of observed cases that are -ve\n",
    "     4) Predicted negative rate: proportion of predicted cases that are -ve  \n",
    "     5) accuracy: proportion of predicted results that are correct    \n",
    "     6) precision: proportion of predicted +ve that are correct\n",
    "     7) recall: proportion of true +ve correctly identified\n",
    "     8) f1: harmonic mean of precision and recall\n",
    "     9) sensitivity: Same as recall\n",
    "    10) specificity: Proportion of true -ve identified:        \n",
    "    11) positive likelihood: increased probability of true +ve if test +ve\n",
    "    12) negative likelihood: reduced probability of true +ve if test -ve\n",
    "    13) false positive rate: proportion of false +ves in true -ve patients\n",
    "    14) false negative rate: proportion of false -ves in true +ve patients\n",
    "    15) true positive rate: Same as recall\n",
    "    16) true negative rate\n",
    "    17) positive predictive value: chance of true +ve if test +ve\n",
    "    18) negative predictive value: chance of true -ve if test -ve\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Converts list to NumPy arrays\n",
    "    if type(observed) == list:\n",
    "        observed = np.array(observed)\n",
    "    if type(predicted) == list:\n",
    "        predicted = np.array(predicted)\n",
    "    \n",
    "    # Calculate accuracy scores\n",
    "    observed_positives = observed == 1\n",
    "    observed_negatives = observed == 0\n",
    "    predicted_positives = predicted == 1\n",
    "    predicted_negatives = predicted == 0\n",
    "    \n",
    "    true_positives = (predicted_positives == 1) & (observed_positives == 1)\n",
    "    \n",
    "    false_positives = (predicted_positives == 1) & (observed_positives == 0)\n",
    "    \n",
    "    true_negatives = (predicted_negatives == 1) & (observed_negatives == 1)\n",
    "    \n",
    "    accuracy = np.mean(predicted == observed)\n",
    "    \n",
    "    precision = (np.sum(true_positives) /\n",
    "                 (np.sum(true_positives) + np.sum(false_positives)))\n",
    "        \n",
    "    recall = np.sum(true_positives) / np.sum(observed_positives)\n",
    "    \n",
    "    sensitivity = recall\n",
    "    \n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    \n",
    "    specificity = np.sum(true_negatives) / np.sum(observed_negatives)\n",
    "    \n",
    "    positive_likelihood = sensitivity / (1 - specificity)\n",
    "    \n",
    "    negative_likelihood = (1 - sensitivity) / specificity\n",
    "    \n",
    "    false_positive_rate = 1 - specificity\n",
    "    \n",
    "    false_negative_rate = 1 - sensitivity\n",
    "    \n",
    "    true_positive_rate = sensitivity\n",
    "    \n",
    "    true_negative_rate = specificity\n",
    "    \n",
    "    positive_predictive_value = (np.sum(true_positives) / \n",
    "                                 np.sum(observed_positives))\n",
    "    \n",
    "    negative_predictive_value = (np.sum(true_negatives) / \n",
    "                                  np.sum(observed_positives))\n",
    "    \n",
    "    # Create dictionary for results, and add results\n",
    "    results = dict()\n",
    "    \n",
    "    results['observed_positive_rate'] = np.mean(observed_positives)\n",
    "    results['observed_negative_rate'] = np.mean(observed_negatives)\n",
    "    results['predicted_positive_rate'] = np.mean(predicted_positives)\n",
    "    results['predicted_negative_rate'] = np.mean(predicted_negatives)\n",
    "    results['accuracy'] = accuracy\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1'] = f1\n",
    "    results['sensitivity'] = sensitivity\n",
    "    results['specificity'] = specificity\n",
    "    results['positive_likelihood'] = positive_likelihood\n",
    "    results['negative_likelihood'] = negative_likelihood\n",
    "    results['false_positive_rate'] = false_positive_rate\n",
    "    results['false_negative_rate'] = false_negative_rate\n",
    "    results['true_positive_rate'] = true_positive_rate\n",
    "    results['true_negative_rate'] = true_negative_rate\n",
    "    results['positive_predictive_value'] = positive_predictive_value\n",
    "    results['negative_predictive_value'] = negative_predictive_value\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to scale data\n",
    "\n",
    "In neural networks it is common to to scale input data 0-1 rather than use standardisation (subtracting mean and dividing by standard deviation) of each feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test):\n",
    "    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = MinMaxScaler()\n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_sc = sc.transform(X_train)\n",
    "    test_sc = sc.transform(X_test)\n",
    "    \n",
    "    return train_sc, test_sc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)\n",
    "data.drop('PassengerId', inplace=True, axis=1)\n",
    "X = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\n",
    "y = data['Survived'] # y = 'survived' column from 'data'\n",
    "# Convert to NumPy as required for k-fold splits\n",
    "X_np = X.values\n",
    "y_np = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up neural net\n",
    "\n",
    "Here we use the api-based method to set up a TensorFlow neural network. This method allows us\n",
    "to more flexibly define the inputs for each layer (it would not be possible to dynamically change the wide and deep or single final layer using the Sequential method).\n",
    "\n",
    "We will put construction of the neural net into a separate function. We need a function for each option of number of hidden layers as this aspect is not possible to dynamically change using the api-based method.\n",
    "\n",
    "The inputs are connected to up to three hidden layers. Each layer having the number of nodes that is a multiple of the number of inputs (up to x 10), before being connected to an output node that give a probability of being a category. It is an option for the input values to feed directly into the output node (along with the final hidden layer) - known as wide and deep. Another option is to have an additional layer prior to the output layer that contains a single node.\n",
    "\n",
    "It also contains some useful additions (batch normalisation and dropout) as described below.\n",
    "\n",
    "The layers of the network are:\n",
    "\n",
    "1) An input layer (which needs to be defined)\n",
    "\n",
    "2) A fully-connected (dense) layer. This is defined by the number of inputs (the number of input features) and the number of outputs. The number of outputs is a multuiple of the number of inputs (up to 10 x). The output of the layer uses ReLU (rectified linear unit) activation. ReLU activation is most common for the inner layers of a neural network. Negative input values are set to zero. Positive input values are left unchanged.\n",
    "\n",
    "3) A batch normalisation layer. This is not usually used for small models, but can increase the speed of training for larger models. It is added here as an example of how to include it (in large models all dense layers would be followed by a batch normalisation layer). The layer definition includes the number of inputs to normalise. \n",
    "\n",
    "4) A dropout layer. This layer randomly sets outputs from the preceding layer to zero during training (a different set of outputs is zeroed for each training iteration). This helps prevent over-fitting of the model to the training data. Typically between 0.1 and 0.3 outputs are set to zero (p=0.1 means 10% of outputs are set to zero).\n",
    "\n",
    "The above three layers are then repeated up to 3 times.\n",
    "\n",
    "5) The option of a single node final layer\n",
    "\n",
    "6) A final fully connected linear layer of one nodes (more nodes could be used for more classes, in which case use softmax activation and categorical_crossentropy in the loss function). The output of the net is the probability of surviving (usually a probability of >= 0.5 will be classes as ‘survived’). Option to feed the input values into this output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net_1_hidden_layer(number_features, number_nodes = 5, \n",
    "                            dropout_rate = 0.25, wide_and_deep = 0, \n",
    "                            single_final_hidden_layer = 0, \n",
    "                            learning_rate = 0.003):\n",
    "    \n",
    "    #create a network with 1 hidden layer\n",
    "    \n",
    "    # Clear Tensorflow\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Define layers\n",
    "    inputs = layers.Input(shape=number_features)\n",
    "    \n",
    "    dense_1 = layers.Dense(number_features * number_nodes, \n",
    "                           activation = 'relu')(inputs)\n",
    "    norm_1 = layers.BatchNormalization()(dense_1)    \n",
    "    dropout_1 = layers.Dropout(dropout_rate)(norm_1)\n",
    "        \n",
    "    if wide_and_deep == 1:\n",
    "        # Create a layer that concatenates last dense layer and the input layer\n",
    "        concat = layers.Concatenate()([inputs, dropout_1])\n",
    "        \n",
    "        if single_final_hidden_layer == 0:\n",
    "            # wide deep goes to output layer\n",
    "            outputs = layers.Dense(1, activation = 'sigmoid')(concat)\n",
    "\n",
    "        else:\n",
    "            # wide deep goes a final hidden layer with 1 node\n",
    "            single_node = layers.Dense(1, activation = 'sigmoid')(concat)\n",
    "            # then into output layer\n",
    "            outputs = layers.Dense(1, activation = 'sigmoid')(single_node)\n",
    "            \n",
    "    else:\n",
    "        if single_final_hidden_layer == 0:\n",
    "            # not include a final hidden layer with 1 node\n",
    "            outputs = layers.Dense(1, activation = 'sigmoid')(dropout_1)\n",
    "            \n",
    "        else:\n",
    "            # include a final hidden layer with 1 node\n",
    "            single_node = layers.Dense(1, activation = 'sigmoid')(dropout_1)\n",
    "            # then into output layer\n",
    "            outputs = layers.Dense(1, activation = 'sigmoid')(single_node)\n",
    "            \n",
    "    net = Model(inputs, outputs)\n",
    "    \n",
    "    # Compiling model\n",
    "    opt = Adam(lr = learning_rate)\n",
    "    \n",
    "    net.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = opt,\n",
    "                metrics = ['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net_2_hidden_layers(number_features, number_nodes = 5, \n",
    "                             dropout_rate = 0.25, wide_and_deep = 0, \n",
    "                             single_final_hidden_layer = 0, \n",
    "                             learning_rate = 0.003):\n",
    "    \n",
    "    #create a network with 2 hidden layers\n",
    "    \n",
    "    # Clear Tensorflow\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Define layers\n",
    "    inputs = layers.Input(shape=number_features)\n",
    "    \n",
    "    dense_1 = layers.Dense(number_features * number_nodes,\n",
    "                           activation = 'relu')(inputs)\n",
    "    norm_1 = layers.BatchNormalization()(dense_1)    \n",
    "    dropout_1 = layers.Dropout(dropout_rate)(norm_1)\n",
    "    \n",
    "    dense_2 = layers.Dense(number_features * number_nodes, \n",
    "                           activation = 'relu')(dropout_1)\n",
    "    norm_2 = layers.BatchNormalization()(dense_2)\n",
    "    dropout_2 = layers.Dropout(dropout_rate)(norm_2)\n",
    "    \n",
    "    if wide_and_deep == 1:\n",
    "        # Create a layer that concatenates last dense layer and the input layer\n",
    "        concat = layers.Concatenate()([inputs, dropout_2])\n",
    "        \n",
    "        if single_final_hidden_layer == 0:\n",
    "            # wide deep goes to output layer\n",
    "            outputs = layers.Dense(1, activation = 'sigmoid')(concat)\n",
    "\n",
    "        else:\n",
    "            # wide deep goes a final hidden layer with 1 node\n",
    "            single_node = layers.Dense(1, activation = 'sigmoid')(concat)\n",
    "            # then into output layer\n",
    "            outputs = layers.Dense(1, activation = 'sigmoid')(single_node)\n",
    "            \n",
    "    else:\n",
    "        if single_final_hidden_layer == 0:\n",
    "            # not include a final hidden layer with 1 node\n",
    "            outputs = layers.Dense(1, activation = 'sigmoid')(dropout_2)\n",
    "            \n",
    "        else:\n",
    "            # include a final hidden layer with 1 node\n",
    "            single_node = layers.Dense(1, activation = 'sigmoid')(dropout_2)\n",
    "            # then into output layer\n",
    "            outputs = layers.Dense(1, activation = 'sigmoid')(single_node)\n",
    "   \n",
    "    net = Model(inputs, outputs)\n",
    "    \n",
    "    # Compiling model\n",
    "    opt = Adam(lr = learning_rate)\n",
    "    \n",
    "    net.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = opt,\n",
    "                metrics = ['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net_3_hidden_layers(number_features, number_nodes = 5, \n",
    "                             dropout_rate = 0.25, wide_and_deep = 0, \n",
    "                             single_final_hidden_layer = 0, \n",
    "                             learning_rate = 0.003):\n",
    "    \n",
    "    #create a network with 3 hidden layers\n",
    "    \n",
    "    # Clear Tensorflow\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Define layers\n",
    "    inputs = layers.Input(shape=number_features)\n",
    "    \n",
    "    dense_1 = layers.Dense(number_features * number_nodes, \n",
    "                           activation = 'relu')(inputs)\n",
    "    norm_1 = layers.BatchNormalization()(dense_1)    \n",
    "    dropout_1 = layers.Dropout(dropout_rate)(norm_1)\n",
    "    \n",
    "    dense_2 = layers.Dense(number_features * number_nodes, \n",
    "                           activation = 'relu')(dropout_1)\n",
    "    norm_2 = layers.BatchNormalization()(dense_2)\n",
    "    dropout_2 = layers.Dropout(dropout_rate)(norm_2)\n",
    "    \n",
    "    dense_3 = layers.Dense(number_features * number_nodes, \n",
    "                           activation = 'relu')(dropout_2)\n",
    "    norm_3 = layers.BatchNormalization()(dense_3)\n",
    "    dropout_3 = layers.Dropout(dropout_rate)(norm_3)\n",
    "    \n",
    "    if wide_and_deep == 1:\n",
    "        # Create a layer that concatenates last dense layer and the input layer\n",
    "        concat = layers.Concatenate()([inputs, dropout_3])\n",
    "        \n",
    "        if single_final_hidden_layer == 0:\n",
    "            # wide deep goes to output layer\n",
    "            outputs = layers.Dense(1, activation = 'sigmoid')(concat)\n",
    "\n",
    "        else:\n",
    "            # wide deep goes a final hidden layer with 1 node\n",
    "            single_node = layers.Dense(1, activation = 'sigmoid')(concat)\n",
    "            # then into output layer\n",
    "            outputs = layers.Dense(1, activation = 'sigmoid')(single_node)\n",
    "            \n",
    "    else:\n",
    "        if single_final_hidden_layer == 0:\n",
    "            # not include a final hidden layer with 1 node\n",
    "            outputs = layers.Dense(1, activation = 'sigmoid')(dropout_3)\n",
    "            \n",
    "        else:\n",
    "            # include a final hidden layer with 1 node\n",
    "            single_node = layers.Dense(1, activation = 'sigmoid')(dropout_3)\n",
    "            # then into output layer\n",
    "            outputs = layers.Dense(1, activation = 'sigmoid')(single_node)\n",
    "        \n",
    "    net = Model(inputs, outputs)\n",
    "    \n",
    "    # Compiling model\n",
    "    opt = Adam(lr = learning_rate)\n",
    "    \n",
    "    net.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = opt,\n",
    "                metrics = ['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net(number_hidden_layers, number_features, number_nodes, dropout_rate, \n",
    "             wide_and_deep, single_final_hidden_layer, learning_rate):\n",
    "\n",
    "    if number_hidden_layers == 1:\n",
    "        model = make_net_1_hidden_layer(number_features, number_nodes, \n",
    "                                        dropout_rate, wide_and_deep, \n",
    "                                        single_final_hidden_layer,\n",
    "                                        learning_rate)\n",
    "    elif number_hidden_layers == 2:\n",
    "        model = make_net_2_hidden_layers(number_features, number_nodes, \n",
    "                                         dropout_rate, wide_and_deep, \n",
    "                                         single_final_hidden_layer, \n",
    "                                         learning_rate)\n",
    "    elif number_hidden_layers == 3:\n",
    "        model = make_net_3_hidden_layers(number_features, number_nodes, \n",
    "                                         dropout_rate, wide_and_deep, \n",
    "                                         single_final_hidden_layer, \n",
    "                                         learning_rate)\n",
    "    else:\n",
    "        print(f\"User requested {number_hidden_layers} hidden layers, this \"\n",
    "              f\"option is not currently possible, a network will be created \"\n",
    "              f\"with 3 hidden layers.\")\n",
    "        model = make_net_3(number_features, number_nodes, dropout_rate, \n",
    "                           wide_and_deep, single_final_hidden_layer, \n",
    "                           learning_rate)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model with k-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up lists to hold results\n",
    "training_acc_results = []\n",
    "test_acc_results = []\n",
    "\n",
    "# Set up splits\n",
    "skf = StratifiedKFold(n_splits = 5)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "# Loop through the k-fold splits\n",
    "k_counter = 0\n",
    "\n",
    "for train_index, test_index in skf.split(X_np, y_np):\n",
    "    k_counter +=1\n",
    "    print('K_fold {}'.format(k_counter))\n",
    "    \n",
    "    # Get X and Y train/test\n",
    "    X_train, X_test = X_np[train_index], X_np[test_index]\n",
    "    y_train, y_test = y_np[train_index], y_np[test_index]\n",
    "    \n",
    "    # Scale X data\n",
    "    X_train_sc, X_test_sc = scale_data(X_train, X_test)\n",
    "    \n",
    "    # Define network\n",
    "    \n",
    "    # Define network\n",
    "    number_features = X_train_sc.shape[1]\n",
    "    model = make_net(number_hidden_layers, number_features, number_nodes, \n",
    "                     dropout, wide_and_deep, single_final_hidden_layer, \n",
    "                     learning_rate)\n",
    "     \n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(X_train_sc,\n",
    "                        y_train,\n",
    "                        epochs = max_epoch,\n",
    "                        batch_size = calculation_batch_size,\n",
    "                        validation_data = (X_test_sc, y_test),\n",
    "                        verbose = 0)\n",
    "            \n",
    "    ### Test model (print results for each k-fold iteration)\n",
    "    probability = model.predict(X_train_sc)\n",
    "    y_pred_train = probability >= 0.5\n",
    "    y_pred_train = y_pred_train.flatten()\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    training_acc_results.append(accuracy_train)\n",
    "\n",
    "    probability = model.predict(X_test_sc)\n",
    "    y_pred_test = probability >= 0.5\n",
    "    y_pred_test = y_pred_test.flatten()\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "    test_acc_results.append(accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and save as png file a visualisation of the neural network structure\n",
    "\n",
    "This needs a pip or conda install pydot and graphviz (included in the tensorflow_batch environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"{output_folder}/{model_name}\"\n",
    "\n",
    "keras.utils.plot_model(model, f\"{model_path}.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show training and test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show individual accuracies on training data\n",
    "training_acc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show individual accuracies on test data\n",
    "test_acc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean results\n",
    "mean_training = np.mean(training_acc_results)\n",
    "mean_test = np.mean(test_acc_results)\n",
    "\n",
    "# Display each to three decimal places\n",
    "print (f\"mean accuracy (across all {number_kfold} k-folds) for training set: {mean_training}\")\n",
    "print (f\"mean accuracy (across all {number_kfold} k-folds) for test set: {mean_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results: Box Plot\n",
    "\n",
    "Box plots show median (orange line), the second and third quartiles (the box), the range (excluding outliers), and any outliers as 'whisker' points. Outliers, by convention, are conisiered to be any points outside of the quartiles +/- 1.5 times the interquartile range. The limit for outliers may be changed using the optional `whis` argument in the boxplot.\n",
    "\n",
    "Medians tend to be an easy reliable guide to the centre of a distribution (i.e. look at the medians to see whether a fit is improving or not, but also look at the box plot to see how much variability there is).\n",
    "\n",
    "Test sets tend to be more variable in their accuracy measures. Can you think why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set up X data \n",
    "x_for_box = [training_acc_results, test_acc_results]\n",
    "\n",
    "# Set up X labels\n",
    "labels = ['Training', 'Test'] \n",
    "\n",
    "# Set up figure\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "# Add subplot (can be used to define multiple plots in same figure)\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# Define Box Plot (`widths` is optional)\n",
    "ax1.boxplot(x_for_box, \n",
    "            widths=0.7,\n",
    "            whis=100)\n",
    "\n",
    "# Set X and Y labels\n",
    "ax1.set_xticklabels(labels)\n",
    "ax1.set_ylabel('Accuracy')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorFlow's training history\n",
    "\n",
    "TensorFlow can track the history of training, enabling us to examine performance against training and test sets over time. Here we will use the same model as above, but without k-fold validation and with history tracking.\n",
    "`history` is a dictionary containing data collected during training. Let's take a look at the keys in this dictionary (these are the metrics monitored during training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Test accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting model weights\n",
    "\n",
    "Here we show how weights for a layer can be extracted from the model if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = hidden1.get_weights() # Biases are not used in this model\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating model\n",
    "\n",
    "If a test set is not used to evaluate in training, or if there is an independnent test set, evaluation may be quickly performed using the `evaluate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_sc, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
